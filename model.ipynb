{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 data_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Folder for YOLOv5 tests\n",
    "\"\"\"\n",
    "data_folder = \"./data-cxywh\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_images_with_labels(image_files, data_folder):\n",
    "    \"\"\"Filters image files to include only those with corresponding label files.\"\"\"\n",
    "    valid_images = []\n",
    "    for img_file in image_files:\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        if os.path.exists(os.path.join(data_folder, label_file)):\n",
    "            valid_images.append((img_file, label_file))\n",
    "    return valid_images\n",
    "\n",
    "image_files = [f for f in os.listdir(data_folder) if f.endswith(\".jpg\")]\n",
    "filtered_images = filter_images_with_labels(image_files, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = random.sample(filtered_images, min(10, len(filtered_images)))\n",
    "\n",
    "def plot_annotated_images(image_files):\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(28, 8))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for idx, (img_file, lbl_file) in enumerate(image_files):\n",
    "        img_path = os.path.join(data_folder, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = img.shape\n",
    "        \n",
    "        with open(os.path.join(data_folder, lbl_file), \"r\") as f:\n",
    "            boxes = f.readlines()\n",
    "        \n",
    "        axs[idx].imshow(img)\n",
    "        for box in boxes:\n",
    "            class_id, x_center, y_center, box_width, box_height = map(float, box.strip().split())\n",
    "            x_center *= width\n",
    "            y_center *= height\n",
    "            box_width *= width\n",
    "            box_height *= height\n",
    "            x_min = x_center - box_width / 2\n",
    "            y_min = y_center - box_height / 2\n",
    "            \n",
    "            axs[idx].add_patch(plt.Rectangle((x_min, y_min), box_width, box_height, edgecolor='red', facecolor='none'))\n",
    "            axs[idx].text(x_min, y_min - 5, f\"{int(class_id) + 1}\", color='red', fontsize=8, verticalalignment='bottom')\n",
    "\n",
    "        axs[idx].axis(\"off\")\n",
    "\n",
    "    for j in range(idx + 1, 10):\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_annotated_images(random_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_label = {}\n",
    "\n",
    "for idx, (_, lbl_file) in enumerate(filtered_images):\n",
    "    with open(os.path.join(data_folder, lbl_file), 'r') as label:\n",
    "        labels = label.readlines()\n",
    "        \n",
    "    labels = [int(l.strip().split()[0]) for l in labels]\n",
    "    for label in labels:\n",
    "        if label in freq_label:\n",
    "            freq_label[label] += 1\n",
    "        else:\n",
    "            freq_label[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def plot_freq_map(freq_map:Dict, x_label:str='Labels', y_label:str='Frequency', title:str='Frequency of Labels'):\n",
    "    keys = sorted(freq_map.keys())\n",
    "    values = [freq_map[key] for key in keys]\n",
    "    total = sum(values)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.bar(keys, values)\n",
    "    plt.xticks(range(min(keys), max(keys) + 1))\n",
    "    plt.gca().text(0.95, 0.95, f'Total labels: {total}', ha='right', va='top', transform=plt.gca().transAxes,\n",
    "                   bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_freq_map(freq_map=freq_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv5 Only\n",
    "\n",
    "<img src=\"yolov5-model-structure.avif\" alt=\"YOLOv5 Model Structure\" width=\"50%\">\n",
    "\n",
    "train, test, validation split: 60, 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM_EPOCHS_YOLO = 1\n",
    "BATCH_SIZE_YOLO = 18\n",
    "\n",
    "yolo_data_folder = './yolo-data'\n",
    "os.makedirs(yolo_data_folder, exist_ok=True)\n",
    "\n",
    "model = YOLO('yolov5s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, temp_images = train_test_split(filtered_images, test_size=0.4, random_state=42)\n",
    "val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
    "\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'train', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'val', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'test', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(yolo_data_folder, 'test', 'labels'), exist_ok=True)\n",
    "\n",
    "for img_file, label_file in train_images:\n",
    "    shutil.copy(os.path.join(data_folder, img_file), os.path.join(yolo_data_folder, 'train', 'images'))\n",
    "    shutil.copy(os.path.join(data_folder, label_file), os.path.join(yolo_data_folder, 'train', 'labels'))\n",
    "\n",
    "for img_file, label_file in val_images:\n",
    "    shutil.copy(os.path.join(data_folder, img_file), os.path.join(yolo_data_folder, 'val', 'images'))\n",
    "    shutil.copy(os.path.join(data_folder, label_file), os.path.join(yolo_data_folder, 'val', 'labels'))\n",
    "\n",
    "for img_file, label_file in test_images:\n",
    "    shutil.copy(os.path.join(data_folder, img_file), os.path.join(yolo_data_folder, 'test', 'images'))\n",
    "    shutil.copy(os.path.join(data_folder, label_file), os.path.join(yolo_data_folder, 'test', 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./runs'):\n",
    "    shutil.rmtree('./runs')\n",
    "history = model.train(data='yolo.yaml', epochs=NUM_EPOCHS_YOLO, batch=BATCH_SIZE_YOLO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_RESULTS_FOLDER = './runs/detect/train'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(YOLO_RESULTS_FOLDER, 'results.csv'))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "ax1.plot(df['epoch'], df['train/box_loss'], label='Train Box Loss', color='blue')\n",
    "ax1.plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Train and Validation Box Losses Over Epochs')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(df['epoch'], df['train/cls_loss'], label='Train Cls Loss', color='green')\n",
    "ax2.plot(df['epoch'], df['val/cls_loss'], label='Val Cls Loss', color='orange')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Train and Validation Classification Losses Over Epochs')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_path = os.path.join(yolo_data_folder, 'test', 'images')\n",
    "\n",
    "model_weights = os.path.join(YOLO_RESULTS_FOLDER, 'weights/best.pt')\n",
    "trained_model = YOLO(model_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def plot_annotated_images_with_predictions(test_images, trained_model):\n",
    "    img_file, lbl_file = choice(test_images)\n",
    "    img_file = os.path.join(yolo_data_folder, 'test', 'images', img_file)\n",
    "    lbl_file = os.path.join(yolo_data_folder, 'test', 'labels', lbl_file)\n",
    "    \n",
    "    img = cv2.imread(img_file)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image {img_file}\")\n",
    "        return\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    with open(lbl_file, \"r\") as f:\n",
    "        ground_truth_boxes = f.readlines()\n",
    "    \n",
    "    results = trained_model(img_file)\n",
    "    predictions = results[0].boxes.xywh.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    ax1.imshow(img_rgb)\n",
    "    ax1.set_title(\"Ground Truth\")\n",
    "    \n",
    "    for gt_box in ground_truth_boxes:\n",
    "        class_id, x_center, y_center, box_width, box_height = map(float, gt_box.strip().split())\n",
    "        x_center *= width\n",
    "        y_center *= height\n",
    "        box_width *= width\n",
    "        box_height *= height\n",
    "        x_min = x_center - box_width / 2\n",
    "        y_min = y_center - box_height / 2\n",
    "        ax1.add_patch(plt.Rectangle((x_min, y_min), box_width, box_height, edgecolor='g', facecolor='none'))\n",
    "        ax1.text(x_min, y_min - 5, f\"{int(class_id) + 1}\", color='g', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    ax2.imshow(img_rgb)\n",
    "    ax2.set_title(\"Prediction\")\n",
    "    \n",
    "    for pred_box, confidence, class_id in zip(predictions, confidences, class_ids):\n",
    "        x_center, y_center, width_pred, height_pred = pred_box\n",
    "        \n",
    "        x_min = x_center - width_pred / 2\n",
    "        y_min = y_center - height_pred / 2\n",
    "        \n",
    "        ax2.add_patch(plt.Rectangle((x_min, y_min), width_pred, height_pred, edgecolor='r', facecolor='none'))\n",
    "        ax2.text(x_min, y_min - 5, f\"{int(class_id) + 1}\\n({confidence:.2f})\", color='r', fontsize=8, verticalalignment='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "for _ in range(5):\n",
    "    plot_annotated_images_with_predictions(test_images, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def iou_score(box_a, box_b):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    x_min_a, y_min_a, width_a, height_a = box_a\n",
    "    x_min_b, y_min_b, width_b, height_b = box_b\n",
    "\n",
    "    x_max_a, y_max_a = x_min_a + width_a, y_min_a + height_a\n",
    "    x_max_b, y_max_b = x_min_b + width_b, y_min_b + height_b\n",
    "\n",
    "    inter_x_min = max(x_min_a, x_min_b)\n",
    "    inter_y_min = max(y_min_a, y_min_b)\n",
    "    inter_x_max = min(x_max_a, x_max_b)\n",
    "    inter_y_max = min(y_max_a, y_max_b)\n",
    "\n",
    "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
    "    area_a = width_a * height_a\n",
    "    area_b = width_b * height_b\n",
    "\n",
    "    union_area = area_a + area_b - inter_area\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "def evaluate_model(test_images, trained_model, iou_threshold=0.5):\n",
    "    ious, y_true, y_pred = [], [], []\n",
    "\n",
    "    for img_file, lbl_file in test_images:\n",
    "        img_path = os.path.join(yolo_data_folder, 'test', 'images', img_file)\n",
    "        lbl_path = os.path.join(yolo_data_folder, 'test', 'labels', lbl_file)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Error: Could not load image {img_path}\")\n",
    "            return\n",
    "        \n",
    "        height, width, _ = img.shape\n",
    "        \n",
    "        with open(lbl_path, \"r\") as f:\n",
    "            ground_truth_boxes = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "        \n",
    "        results = trained_model(img_path)\n",
    "        predictions = results[0].boxes.xywh.cpu().numpy()\n",
    "        pred_classes = results[0].boxes.cls.cpu().numpy()\n",
    "        \n",
    "        for gt_class, x_center, y_center, box_width, box_height in ground_truth_boxes:\n",
    "            x_center *= width\n",
    "            y_center *= height\n",
    "            box_width *= width\n",
    "            box_height *= height\n",
    "            gt_box = [(x_center - box_width / 2), (y_center - box_height / 2), box_width, box_height]\n",
    "            max_iou, matched_pred_class = 0, -1\n",
    "            \n",
    "            for pred_box, pred_class in zip(predictions, pred_classes):\n",
    "                x_center, y_center, width_pred, height_pred = pred_box\n",
    "                \n",
    "                x_min = x_center - width_pred / 2\n",
    "                y_min = y_center - height_pred / 2\n",
    "                \n",
    "                pred_box = [x_min, y_min, width_pred, height_pred]\n",
    "                \n",
    "                iou = iou_score(gt_box, pred_box)\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    matched_pred_class = int(pred_class)\n",
    "            \n",
    "            ious.append(max_iou)\n",
    "            y_true.append(int(gt_class))\n",
    "            y_pred.append(matched_pred_class if max_iou >= iou_threshold else -1)\n",
    "    \n",
    "    ious = np.array(ious)\n",
    "    avg_iou = np.mean(ious)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'avg_iou': avg_iou,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "metrics = evaluate_model(test_images, trained_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names, metrics):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    \n",
    "    metrics_text = (\n",
    "        f\"Average IoU: {metrics['avg_iou']:.2f}\\n\"\n",
    "        f\"Precision: {metrics['precision']:.2f}\\n\"\n",
    "        f\"Recall: {metrics['recall']:.2f}\\n\"\n",
    "        f\"F1 Score: {metrics['f1_score']:.2f}\"\n",
    "    )\n",
    "    \n",
    "    plt.gca().text(1.05, 1.05, metrics_text, transform=plt.gca().transAxes,\n",
    "                   fontsize=12, verticalalignment='top',\n",
    "                   bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "class_names = [f\"{i + 1}\" for i in range(len(metrics['confusion_matrix']))]\n",
    "plot_confusion_matrix(metrics['confusion_matrix'], class_names, metrics)\n",
    "\n",
    "del trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOUGH TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoughTransform(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 3\n",
    "    )\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    processed = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    \n",
    "    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    output_image = np.full_like(gray, 50)\n",
    "    \n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Set minimum and maximum size thresholds for Braille dots\n",
    "        if 3 <= w <= 7 and 3 <= h <= 7:  # Size range for Braille dots\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area < 50:\n",
    "                confidence_intensity = int(255 - (area / 50) * 205)  # From 50 (low confidence) to 255 (high confidence)\n",
    "                cv2.drawContours(output_image, [contour], -1, confidence_intensity, thickness=cv2.FILLED)\n",
    "    \n",
    "    return gray, blurred, thresh, processed, output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEST HOUGH TRANSFORM\n",
    "\"\"\"\n",
    "def test_hough_transform(image_path):\n",
    "    test_image = cv2.imread(image_path)\n",
    "    \n",
    "    gray, blurred, thresh, processed, output_image = HoughTransform(test_image)\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "    plt.title(\"Grayscale Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.imshow(blurred, cmap='gray')\n",
    "    plt.title(\"Blurred Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.imshow(thresh, cmap='gray')\n",
    "    plt.title(\"Thresholded Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(processed, cmap='gray')\n",
    "    plt.title(\"Processed Image (Morphology)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(output_image, cmap='gray')\n",
    "    plt.title(\"Detected Braille Dots (Confidence-Based Intensity)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_hough_transform(os.path.join(data_folder, 'IMG_20190715_112947.labeled_0_512.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRAILLE SPOTTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_conv_block(model, skip=False, batch_size=16, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    layer2 = None\n",
    "    if skip:\n",
    "        layer2 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, layer2)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=3, skip=False) -> None:\n",
    "        super(ConvolutionBlock, self).__init__()\n",
    "        self.skip = skip\n",
    "        \n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=in_channels,\n",
    "                      kernel_size=1,\n",
    "                      stride=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.seq2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=in_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.seq_g = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=in_channels,\n",
    "                      kernel_size=1,\n",
    "                      stride=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.silu = nn.SiLU(inplace=True)\n",
    "        \n",
    "    def forward(self, Xf, Xg=None):\n",
    "        xf = self.seq1(Xf)\n",
    "        xf = self.seq2(xf)\n",
    "        if self.skip and Xg is not None:\n",
    "            xg = self.seq_g(Xg)\n",
    "            xf = torch.concat([xf, xg], dim=1)\n",
    "        xf = self.silu(xf)\n",
    "        return xf\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_conv_block(ConvolutionBlock()) # no skip\n",
    "test_conv_block(ConvolutionBlock(skip=True), skip=True) # with skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_up_conv_block(model, ch_in=512, batch_size=16, imgsz=20):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    image_tensor = torch.randn(batch_size, ch_in, imgsz, imgsz).to(device)\n",
    "    output = model(image_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, scale_factor=4):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "                                nn.Upsample(scale_factor=scale_factor),\n",
    "                                nn.Conv2d(ch_in, ch_out,\n",
    "                                         kernel_size=3,stride=1,\n",
    "                                         padding=1, bias=True),\n",
    "                                nn.BatchNorm2d(ch_out),\n",
    "                                nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_up_conv_block(UpConvBlock(ch_in=512, ch_out=1024, scale_factor=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_retention(model, x1_channels=512, x2_channels=512, batch_size=16, imgsz=20):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, x1_channels, imgsz, imgsz).to(device)\n",
    "    layer2 = torch.randn(batch_size, x2_channels, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, layer2)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "\n",
    "class RetentionLayer(nn.Module):\n",
    "    def __init__(self, x1_channels, x2_channels, mid_channels):\n",
    "        super(RetentionLayer, self).__init__()\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=x1_channels,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels)\n",
    "        )\n",
    "        self.seq2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=x2_channels,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=mid_channels,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X1, X2):\n",
    "        x1 = self.seq1(X1)\n",
    "        x2 = self.seq2(X2)\n",
    "        S = self.relu(x1 + x2)\n",
    "        S = self.attention(S)\n",
    "        return X1 * S\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_retention(RetentionLayer(512, 512, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BACKBONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_backbone(model, batch_size=16, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    image_tensor = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(image_tensor)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLO Backbone:\n",
    "    \n",
    "    output feature map: (batch_size, 3, 256, 256)\n",
    "    \"\"\"\n",
    "    def __init__(self, model=\"v11\"):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        if model == \"v11\":\n",
    "            self.yolo = YOLO('yolo11n.pt').model\n",
    "            self.backbone = nn.Sequential(self.yolo)\n",
    "        else:\n",
    "            raise ValueError(\"Only v11 is implemented\")\n",
    "            \n",
    "        self.grid_size = 256\n",
    "        \n",
    "        self.up_conv1 = UpConvBlock(ch_in=144, ch_out=144, scale_factor=2)\n",
    "        self.combination1 = RetentionLayer(144, 144, 32)\n",
    "        \n",
    "        self.up_conv2 = UpConvBlock(ch_in=144, ch_out=144, scale_factor=4)\n",
    "        self.combination2 = RetentionLayer(144, 144, 32)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm2d(144)\n",
    "        \n",
    "        self.seq1 = nn.Sequential(\n",
    "            UpConvBlock(ch_in=144, ch_out=12, scale_factor=2),\n",
    "            ConvolutionBlock(in_channels=12, skip=True),\n",
    "            ConvolutionBlock(in_channels=12, skip=True),\n",
    "            UpConvBlock(ch_in=12, ch_out=6, scale_factor=2),\n",
    "            ConvolutionBlock(in_channels=6, skip=True),\n",
    "            ConvolutionBlock(in_channels=6, skip=True),\n",
    "            UpConvBlock(ch_in=6, ch_out=3, scale_factor=2),\n",
    "            ConvolutionBlock(in_channels=3, skip=True),\n",
    "        )\n",
    "        \n",
    "        self._freeze_layers()\n",
    "        \n",
    "        \n",
    "    def _freeze_layers(self):\n",
    "        for k, v in self.backbone.named_parameters():\n",
    "            v.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        backbone_out = self.backbone(x)\n",
    "        \"\"\"\n",
    "        While evalueation: tuple\n",
    "        While training: list\n",
    "        \"\"\"\n",
    "        if isinstance(backbone_out, tuple):          # tuple([batch_size, 84, 1344], list([batch_size, 144, 32, 32], [batch_size, 144, 16, 16], [batch_size, 144, 8, 8]))\n",
    "            _, segmentation_head = backbone_out[0], backbone_out[1]\n",
    "            x1, x2, x3 = segmentation_head\n",
    "        if isinstance(backbone_out, list):           # list([batch_size, 144, 32, 32], [batch_size, 144, 16, 16], [batch_size, 144, 8, 8])\n",
    "            x1, x2, x3 = backbone_out\n",
    "        \n",
    "        x2 = self.up_conv1(x2)\n",
    "        x1 = self.combination1(x1, x2)\n",
    "        \n",
    "        x3 = self.up_conv2(x3)\n",
    "        x1 = self.combination2(x1, x3)\n",
    "        backbone_out = self.batch_norm(x1)            # (batch_size, 144, 32, 32)\n",
    "        \n",
    "        if isinstance(backbone_out, list):\n",
    "            backbone_out = torch.stack(backbone_out)\n",
    "        out = self.seq1(backbone_out)                 # (batch_size, 3, 256, 256)\n",
    "        \n",
    "        return out                                    # (batch_size, 3, 256, 256)\n",
    "    \n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_backbone(Backbone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hough Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_HoughModule(model, batch_size=16, imgsz=512):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, imgsz=imgsz)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class HoughModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HoughModule, self).__init__()\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=6,\n",
    "                      out_channels=3,\n",
    "                      kernel_size=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def create_bbox_mask(self, bbox_masks, batch_size, imgsz):\n",
    "        batch_masks = []\n",
    "        for i in range(batch_size):\n",
    "            mask = np.zeros((imgsz, imgsz), dtype=np.uint8)\n",
    "            if bbox_masks is not None and len(bbox_masks) > 0:\n",
    "                for bbox in bbox_masks[i]:\n",
    "                    bbox = bbox.cpu()\n",
    "                    points = np.array(bbox).reshape(-1, 2).astype(np.int32)\n",
    "                    cv2.fillPoly(mask, [points], 1)\n",
    "            batch_masks.append(mask)\n",
    "        \n",
    "        batch_masks = torch.tensor(np.stack(batch_masks), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        return batch_masks\n",
    "    \n",
    "    def forward(self, image, bbox_masks=None, imgsz=256):\n",
    "        image_np = image.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
    "        hough_mask = []\n",
    "        for img in image_np:\n",
    "            _, _, _, _, mask = HoughTransform(img)\n",
    "            mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
    "            hough_mask.append(mask)\n",
    "        hough_mask = torch.tensor(np.stack(hough_mask), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        batch_size = image.size(0)\n",
    "        bbox_mask = self.create_bbox_mask(bbox_masks, batch_size, imgsz)\n",
    "        \n",
    "        combined_mask = (hough_mask + bbox_mask) / 2.0\n",
    "        \n",
    "        transformed_image = image * combined_mask\n",
    "        out = self.seq1(transformed_image)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_HoughModule(HoughModule())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBOX Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_BBoxPredictBlock(model, batch_size=16, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    layer2 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, layer2)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class BBoxPredictBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BBoxPredictBlock, self).__init__()\n",
    "        \n",
    "        self.combination = RetentionLayer(\n",
    "            x1_channels=3,\n",
    "            x2_channels=3,\n",
    "            mid_channels=6\n",
    "        )\n",
    "        \n",
    "        self.down_conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv1 = ConvolutionBlock(in_channels=64, skip=True)\n",
    "        \n",
    "        self.down_conv2 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv2 = ConvolutionBlock(in_channels=128, skip=True)\n",
    "        \n",
    "        self.down_conv3 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv3 = ConvolutionBlock(in_channels=256, skip=True)\n",
    "        \n",
    "        self.down_conv4 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=512,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv4 = ConvolutionBlock(in_channels=512, skip=True)\n",
    "        \n",
    "        self.predict_head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=9 * 3, # [class_probability x1 y1 x2 y2 x3 y3 x4 y4] * 3\n",
    "                      kernel_size=1,      # Every one of the 16 X 16 \"block\" outputs 3 cells with the OBB coords\n",
    "                      stride=1),\n",
    "            nn.BatchNorm2d(9 * 3),\n",
    "            nn.Softmax(dim=1),            # Make the predicitons positive\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, img):\n",
    "        \"\"\"\n",
    "        x, img: (batch_size, 3, 256, 256)\n",
    "        \"\"\"\n",
    "        combined = self.combination(x, img)                               # (batch_size, 3, 256, 256)\n",
    "        \n",
    "        x1 = self.down_conv1(combined)                                    # (batch_size, 64, 128, 128)\n",
    "        x1 = self.conv1(x1)                                               # (batch_size, 64, 128, 128)\n",
    "        \n",
    "        x2 = self.down_conv2(x1)                                          # (batch_size, 128, 64, 64)\n",
    "        x2 = self.conv2(x2)                                               # (batch_size, 128, 64, 64)\n",
    "        \n",
    "        x3 = self.down_conv3(x2)                                          # (batch_size, 256, 32, 32)\n",
    "        x3 = self.conv3(x3)                                               # (batch_size, 256, 32, 32)\n",
    "        \n",
    "        x4 = self.down_conv4(x3)                                          # (batch_size, 512, 16, 16)\n",
    "        x4 = self.conv4(x4)                                               # (batch_size, 512, 16, 16)\n",
    "        \n",
    "        predictions = self.predict_head(x4)                               # (batch_size, 9 * 3, 16, 16)\n",
    "        \n",
    "        predictions = predictions.view(predictions.size(0), 9, 3, 16, 16) # (batch_size, 9, 3, 16, 16) -> 3 * 16 * 16 predicted boxes (OBB)\n",
    "        predictions = predictions.permute(0, 2, 3, 4, 1)                  # (batch_size, 3, 16, 16, 9) -> bboxes at last\n",
    "        predictions = predictions.reshape(predictions.size(0), -1, 9)     # (batch_size, 3 * 16 * 16, 9) == (batch_size, 768, 9)\n",
    "        return predictions\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_BBoxPredictBlock(BBoxPredictBlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMS and Box Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "\n",
    "def iou_skewed(box, boxes):\n",
    "    def area(box):\n",
    "        x = box[0::2]\n",
    "        y = box[1::2]\n",
    "        return 0.5 * abs((x[0] * y[1] + x[1] * y[2] + x[2] * y[3] + x[3] * y[0])\n",
    "                       - (x[1] * y[0] + x[2] * y[1] + x[3] * y[2] + x[0] * y[3]))\n",
    "    \n",
    "    def find_intersection(box1, box2):\n",
    "        try:\n",
    "            box1 = [(box1[i], box1[i + 1]) for i in range(0, len(box1), 2)]\n",
    "            box1.append(box1[0])\n",
    "            box2 = [(box2[i], box2[i + 1]) for i in range(0, len(box2), 2)]\n",
    "            box2.append(box2[0])\n",
    "            box1 = shapely.geometry.Polygon(box1)\n",
    "            box2 = shapely.geometry.Polygon(box2)\n",
    "            if not box1.is_valid:\n",
    "                box1 = box1.buffer(0)\n",
    "            if not box2.is_valid:\n",
    "                box2 = box2.buffer(0)\n",
    "            \n",
    "            intersection = box1.intersection(box2)\n",
    "            return intersection.area if intersection.is_valid else 1e-7\n",
    "        except:\n",
    "            return 1e-7\n",
    "\n",
    "    \n",
    "    ious = []\n",
    "    box_area = area(box)\n",
    "    \n",
    "    for b in boxes:\n",
    "        b_area = area(b)\n",
    "        \n",
    "        inter_area = find_intersection(box, b)\n",
    "        \n",
    "        union_area = box_area + b_area - inter_area\n",
    "        iou = inter_area / union_area if union_area != 0 else 0\n",
    "        ious.append(iou)\n",
    "        \n",
    "    return torch.Tensor(ious)\n",
    "\n",
    "def filter_bboxes(predictions, threshold=0.5, iou_threshold=0.1):\n",
    "    batch_size = 0\n",
    "    if isinstance(predictions, list):\n",
    "        batch_size = len(predictions)\n",
    "        predictions = np.array(predictions)\n",
    "    else:\n",
    "        batch_size = predictions.size(0)\n",
    "    \n",
    "    box_probs = predictions[..., 0]\n",
    "    box_coords = predictions[..., 1:]\n",
    "    \n",
    "    selected_boxes = []\n",
    "    \n",
    "    # for each boxes in an image\n",
    "    for idx in range(batch_size):\n",
    "        # apply threshold\n",
    "        mask = box_probs[idx] >= threshold\n",
    "        \n",
    "        # 768 cooradinates with probabilities >= threshold\n",
    "        filtered_coords = box_coords[idx][mask]\n",
    "        filtered_probs = box_probs[idx][mask]\n",
    "        \n",
    "        if len(filtered_coords) == 0:\n",
    "            selected_boxes.append(torch.empty((0, 8)))\n",
    "            continue\n",
    "        \n",
    "        keep = []\n",
    "        while filtered_probs.size(0) > 0:\n",
    "            max_idx = filtered_probs.argmax()\n",
    "            best_box = filtered_coords[max_idx]\n",
    "            \n",
    "            # keep the best box and filter out the nearby boxes\n",
    "            keep.append(best_box)\n",
    "            filtered_coords = torch.cat((filtered_coords[:max_idx], filtered_coords[max_idx+1:]), dim=0)\n",
    "            filtered_probs = torch.cat((filtered_probs[:max_idx], filtered_probs[max_idx+1:]), dim=0)\n",
    "            \n",
    "            if len(filtered_coords) == 1:\n",
    "                break\n",
    "            \n",
    "            ious = iou_skewed(best_box, filtered_coords) # (1, 8), (remaining_boxes, 8)\n",
    "            iou_mask = ious < iou_threshold\n",
    "            filtered_coords = filtered_coords[iou_mask]\n",
    "            filtered_probs = filtered_probs[iou_mask]\n",
    "            \n",
    "        selected_boxes.append(torch.stack(keep) if keep else torch.empty((0, 8)))\n",
    "    return selected_boxes\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Testing BBOX Selection:\n",
    "\"\"\"\n",
    "def test_BBoxSelection(model, batch_size=16, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    layer2 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, layer2)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    selected_boxes = filter_bboxes(output)\n",
    "    for idx, image in enumerate(selected_boxes):\n",
    "        print(f\"Image #{idx+1}:\")\n",
    "        for box in image:\n",
    "            print(box)\n",
    "\"\"\"\n",
    "Test:\n",
    "\"\"\"\n",
    "test_BBoxSelection(BBoxPredictBlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Predict Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_ClassPredictBlock(model, batch_size=16, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    layer2 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    output = model(layer1, layer2)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "class ClassPredictBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassPredictBlock, self).__init__()\n",
    "        self.combination = RetentionLayer(\n",
    "            x1_channels=3,\n",
    "            x2_channels=3,\n",
    "            mid_channels=6\n",
    "        )\n",
    "        \n",
    "        self.down_conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv1 = ConvolutionBlock(in_channels=64, skip=True)\n",
    "        \n",
    "        self.down_conv2 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv2 = ConvolutionBlock(in_channels=128, skip=True)\n",
    "        \n",
    "        self.down_conv3 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv3 = ConvolutionBlock(in_channels=256, skip=True)\n",
    "        \n",
    "        self.down_conv4 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=256,\n",
    "            kernel_size=6,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv4 = ConvolutionBlock(in_channels=256, skip=True)\n",
    "        \n",
    "        self.predict_head = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 16 * 16,\n",
    "                      out_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1024,\n",
    "                      out_features=63),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, img):\n",
    "        \"\"\"\n",
    "        x, img: (batch_size, 3, 256, 256)\n",
    "        \"\"\"\n",
    "        combined = self.combination(x, img)                               # (batch_size, 3, 256, 256)\n",
    "        \n",
    "        x1 = self.down_conv1(combined)                                    # (batch_size, 64, 128, 128)\n",
    "        x1 = self.conv1(x1)                                               # (batch_size, 64, 128, 128)\n",
    "        \n",
    "        x2 = self.down_conv2(x1)                                          # (batch_size, 128, 64, 64)\n",
    "        x2 = self.conv2(x2)                                               # (batch_size, 128, 64, 64)\n",
    "        \n",
    "        x3 = self.down_conv3(x2)                                          # (batch_size, 256, 32, 32)\n",
    "        x3 = self.conv3(x3)                                               # (batch_size, 256, 32, 32)\n",
    "        \n",
    "        x4 = self.down_conv4(x3)                                          # (batch_size, 256, 16, 16)\n",
    "        x4 = self.conv4(x4)                                               # (batch_size, 256, 16, 16)\n",
    "        \n",
    "        x4 = torch.flatten(x4, start_dim=1)                               # (batch_size, 256 * 16 * 16, 1)\n",
    "        \n",
    "        predictions = self.predict_head(x4)                               # (batch_size, 9 * 3, 16, 16)\n",
    "        return predictions\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_ClassPredictBlock(ClassPredictBlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing Models:\n",
    "\"\"\"\n",
    "def test_BrailleSpottingModel(model, prev_bbox=False, batch_size=6, imgsz=256):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    layer1 = torch.randn(batch_size, 3, imgsz, imgsz).to(device)\n",
    "    layer2 = None\n",
    "    if prev_bbox:\n",
    "        layer2 = torch.randn(batch_size, 768, 9).to(device)\n",
    "    bbox_predictions, class_predictions = model(layer1, layer2)\n",
    "    print(\"Output shape:\", bbox_predictions.shape, class_predictions.shape)\n",
    "\n",
    "class BrailleSpottingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrailleSpottingModel, self).__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.hough_module = HoughModule()\n",
    "        self.bbox_predict_model = BBoxPredictBlock()\n",
    "        self.class_predict_model = ClassPredictBlock()\n",
    "    \n",
    "    def forward(self, batch, prev_bbox_predictions=None):\n",
    "        backbone_predictions = self.backbone(batch)\n",
    "        hough_predictions = None\n",
    "        if prev_bbox_predictions is None:\n",
    "            hough_predictions = self.hough_module(batch)\n",
    "        else:\n",
    "            filtered_bboxes = filter_bboxes(prev_bbox_predictions)\n",
    "            hough_predictions = self.hough_module(batch, filtered_bboxes)\n",
    "        \n",
    "        bbox_predictions = self.bbox_predict_model(backbone_predictions, hough_predictions)\n",
    "        class_predictions = self.class_predict_model(backbone_predictions, hough_predictions)\n",
    "        \n",
    "        \n",
    "        return bbox_predictions, class_predictions\n",
    "\n",
    "\"\"\"\n",
    "TEST\n",
    "\"\"\"\n",
    "test_BrailleSpottingModel(BrailleSpottingModel())\n",
    "test_BrailleSpottingModel(BrailleSpottingModel(), prev_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        label_path = os.path.join(self.root_dir, image_name.replace('.jpg', '.txt'))\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image = image.transpose(2, 0, 1)\n",
    "        # image = torch.Tensor(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        bboxes = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                data = line.strip().split()\n",
    "                bbox = list(map(float, data))\n",
    "                bboxes.append(bbox)\n",
    "        \n",
    "        return image, bboxes\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, bboxes = zip(*batch)\n",
    "    images = torch.stack([torch.tensor(img) for img in images])\n",
    "    return images, list(bboxes)\n",
    "\n",
    "def create_dataloaders(root_dir, batch_size, transform=None):\n",
    "    dataset = CustomDataset(root_dir, transform=transform)\n",
    "    \n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.05 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(bbox_predictions, class_predictions, ground_truth):\n",
    "    def dice_loss(bbox_predictions, gt):\n",
    "        def find_intersection(box1, box2):\n",
    "            try:\n",
    "                box1 = [(box1[i], box1[i + 1]) for i in range(0, len(box1), 2)]\n",
    "                box1.append(box1[0])\n",
    "                box2 = [(box2[i], box2[i + 1]) for i in range(0, len(box2), 2)]\n",
    "                box2.append(box2[0])\n",
    "                box1 = shapely.geometry.Polygon(box1)\n",
    "                box2 = shapely.geometry.Polygon(box2)\n",
    "                \n",
    "                intersection = box1.intersection(box2)\n",
    "                return intersection.area if intersection.is_valid else 1e-7\n",
    "            except:\n",
    "                return 1e-7\n",
    "\n",
    "        def find_union(box1, box2):\n",
    "            try:\n",
    "                box1 = [(box1[i], box1[i + 1]) for i in range(0, len(box1), 2)]\n",
    "                box1.append(box1[0])\n",
    "                box2 = [(box2[i], box2[i + 1]) for i in range(0, len(box2), 2)]\n",
    "                box2.append(box2[0])\n",
    "                box1 = shapely.geometry.Polygon(box1)\n",
    "                box2 = shapely.geometry.Polygon(box2)\n",
    "                if not box1.is_valid:\n",
    "                    box1 = box1.buffer(0)\n",
    "                if not box2.is_valid:\n",
    "                    box2 = box2.buffer(0)\n",
    "                \n",
    "                union = box1.union(box2)\n",
    "                return union.area if union.is_valid else 1e-7\n",
    "            except:\n",
    "                return 1e-7\n",
    "\n",
    "        batch_size = bbox_predictions.size(0)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            pred_boxes = bbox_predictions[b]                  # (786, 9)\n",
    "            gt_boxes = gt[b]                                  # (N, 9)\n",
    "            \n",
    "            image_loss = 0\n",
    "            valid_preds = pred_boxes[pred_boxes[:, 0] > 0.3]  # Threshold probability at 0.3\n",
    "            for pred_box in valid_preds:\n",
    "                pred_coords = pred_box[1:].reshape(-1).tolist()\n",
    "                max_dice = 0\n",
    "                \n",
    "                for gt_box in gt_boxes:\n",
    "                    gt_coords = gt_box[1:].reshape(-1).tolist()\n",
    "\n",
    "                    intersection = find_intersection(pred_coords, gt_coords)\n",
    "                    union = find_union(pred_coords, gt_coords)\n",
    "\n",
    "                    dice = (2 * intersection) / (union + intersection)\n",
    "                    max_dice = max(max_dice, dice)\n",
    "                \n",
    "                image_loss += (1 - max_dice)\n",
    "            \n",
    "            total_loss += image_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "    \n",
    "    def cross_entropy_loss(class_predictions, gt):\n",
    "        batch_size = class_predictions.shape[0]\n",
    "        total_loss = 0\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            gt_boxes = gt[b]                         # (N, 9)\n",
    "            gt_class_labels = gt_boxes[:, 0].long()  # (N,)\n",
    "            \n",
    "            class_loss = 0\n",
    "            \n",
    "            for gt_class in gt_class_labels:\n",
    "                class_probs = class_predictions[b]   # (63,)\n",
    "                class_loss += nn.functional.cross_entropy(class_probs.unsqueeze(0), gt_class.unsqueeze(0), reduction='sum')\n",
    "            total_loss += class_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "    \n",
    "    loss_dice = dice_loss(bbox_predictions, ground_truth)\n",
    "    loss_cross_entropy = cross_entropy_loss(class_predictions, ground_truth)\n",
    "    LAMBDA = 1\n",
    "    \n",
    "    return loss_cross_entropy + LAMBDA * loss_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def one_epoch(model, train_loader, loss, optimizer, device, prev_bbox_predictions=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    current_bbox_predictions = []\n",
    "\n",
    "    for images, ground_truth in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.float().to(device, non_blocking=True)\n",
    "        ground_truth = [torch.tensor(gt, dtype=torch.float32).to(device, non_blocking=True) for gt in ground_truth]\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            bbox_predictions, class_predictions = model(images, prev_bbox_predictions)\n",
    "            current_bbox_predictions.extend(bbox_predictions)\n",
    "\n",
    "            current_loss = loss(bbox_predictions, class_predictions, ground_truth)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += current_loss.item()\n",
    "\n",
    "        del images, ground_truth, bbox_predictions, class_predictions, current_loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return running_loss / len(train_loader.dataset), current_bbox_predictions\n",
    "\n",
    "def validation(model, val_loader, loss, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, ground_truth in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.float().to(device, non_blocking=True)\n",
    "            ground_truth = [torch.tensor(gt, dtype=torch.float32).to(device, non_blocking=True) for gt in ground_truth]\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                bbox_predictions, class_predictions = model(images)\n",
    "                current_loss = loss(bbox_predictions, class_predictions, ground_truth)\n",
    "\n",
    "            val_loss += current_loss.item()\n",
    "\n",
    "            del images, ground_truth, bbox_predictions, class_predictions, current_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return val_loss / len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = './data-obb'\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 40\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(root_dir=DATASET_FOLDER, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "if 'model' in globals() or 'model' in locals():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = BrailleSpottingModel()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_bbox_predictions = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, prev_bbox_predictions = one_epoch(model, train_loader, loss, optimizer, device, prev_bbox_predictions)\n",
    "    val_loss = validation(model, val_loader, loss, device)\n",
    "    \n",
    "    bbox_predictions = []\n",
    "    batch = []\n",
    "    for idx in range(0, len(prev_bbox_predictions)):\n",
    "        batch.append(prev_bbox_predictions[idx])\n",
    "        idx += 1\n",
    "        if idx % BATCH_SIZE == 0:\n",
    "            bbox_predictions.append(torch.stack(batch))\n",
    "            batch = []\n",
    "            \n",
    "    if len(batch):\n",
    "        bbox_predictions.append(torch.stack(batch))\n",
    "    \n",
    "    prev_bbox_predictions = bbox_predictions\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fybp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
